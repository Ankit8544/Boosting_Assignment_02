{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-01`    What is Gradient Boosting Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Gradient Boosting Regression is a machine learning technique used for regression tasks`, where the goal is to predict a continuous target variable**. It is an ensemble learning method that combines the predictions from multiple individual models (typically decision trees) in a sequential manner.\n",
    "\n",
    "**`Here's how Gradient Boosting Regression works` :**\n",
    "\n",
    "1. **Base Model Creation -** Initially, a simple model is trained on the data. This could be a decision tree, usually a shallow one.\n",
    "\n",
    "2. **Residual Calculation -** The errors, or residuals, from the first model are calculated. These residuals represent the difference between the predicted values and the actual target values.\n",
    "\n",
    "3. **Training of Subsequent Models -** A new model is then trained to predict these residuals. This model is trained to correct the errors made by the previous model.\n",
    "\n",
    "4. **Gradient Descent -** Rather than fitting the new model to the original target values, it is fitted to the residuals. This is done by minimizing a loss function (often the mean squared error) with gradient descent.\n",
    "\n",
    "5. **Combining Predictions -** The predictions from all the models are combined to make the final prediction. The final prediction is the sum of the predictions from all the individual models.\n",
    "\n",
    "6. **Iterative Process -** Steps 2-5 are repeated for a specified number of iterations, or until a stopping criterion is met. Each new model focuses on the errors made by the ensemble of models built so far.\n",
    "\n",
    "**The \"gradient\" in gradient boosting refers to the use of gradient descent optimization algorithm to minimize the loss when adding new models to the ensemble.**\n",
    "\n",
    "Gradient Boosting Regression is known for its high predictive accuracy and robustness against overfitting, especially when using shallow trees as base learners and employing regularization techniques. Popular implementations of Gradient Boosting Regression include XGBoost, LightGBM, and Gradient Boosting Machines (GBM). These algorithms are widely used in various domains, including finance, healthcare, and online advertising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-02`    Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Gradient Boosting` is a powerful machine learning algorithm that is often used for regression problems**. It works by building a series of weak models, typically decision trees, and combining them to form a strong model. Here's a simple implementation of the gradient boosting algorithm from scratch using Python and NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Dataset` :** **For this example, we will use a small dataset with 100 samples and 2 features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate some random data for our example\n",
    "rng = np.random.RandomState(42)\n",
    "x = rng.rand(100, 2)\n",
    "y = 2 * np.sin(x[:, 0] + x[:, 1]) + rng.rand(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Gradient Boosting Algorithm` :** **Here's the implementation of the gradient boosting algorithm from scratch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 350.27\n",
      "R-squared: -1378.49\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, loss='ls'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.loss = loss\n",
    "\n",
    "        self.models = []\n",
    "        self.oof_predictions = None  # initialize oof_predictions to None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "        residual = y.reshape(-1, 1)\n",
    "\n",
    "        # Initialize oof_predictions here with zeros array\n",
    "        self.oof_predictions = np.zeros_like(y)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            # fit a decision tree to the residual\n",
    "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            model.fit(self.X_train, residual)\n",
    "\n",
    "            # predict the residual for the training set\n",
    "            self.oof_predictions += self.learning_rate * model.predict(self.X_train)\n",
    "\n",
    "            # calculate the new residual\n",
    "            if self.loss == 'ls':\n",
    "                residual = y - self.oof_predictions\n",
    "            elif self.loss == 'lad':\n",
    "                residual = np.sign(y - self.oof_predictions) * np.abs(y - self.oof_predictions)\n",
    "            else:\n",
    "                raise ValueError('Invalid loss function')\n",
    "\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sum([model.predict(X) for model in self.models], axis=0)\n",
    "\n",
    "# create a gradient boosting model with 100 trees, a learning rate of 0.1, and a max depth of 3\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "\n",
    "# fit the model to the data\n",
    "gbr.fit(x, y)\n",
    "\n",
    "# make predictions on the training set\n",
    "y_pred = gbr.predict(x)\n",
    "\n",
    "# calculate the mean squared error\n",
    "mse = np.mean((y_pred - y) ** 2)\n",
    "print(f'MSE: {mse:.2f}')\n",
    "\n",
    "# calculate the R-squared score\n",
    "r2 = 1 - np.sum((y_pred - y) ** 2) / np.sum((y - np.mean(y)) ** 2)\n",
    "print(f'R-squared: {r2:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Interpret` :**\n",
    "\n",
    "1. **Mean Squared Error (MSE) -** MSE measures the average squared difference between the predicted values and the actual values. In this case, the MSE value of 350.27 indicates that, on average, the squared difference between the predicted and actual values is quite high. This suggests that the model's predictions deviate significantly from the actual values.\n",
    "\n",
    "2. **R-squared Score (R2) -** R-squared is a measure of how well the model explains the variance in the data. A negative R-squared value, such as -1378.49, is highly unusual and typically indicates a severe problem with the model. R-squared values should range from 0 to 1, where 1 indicates a perfect fit. Negative values indicate that the model performs worse than a horizontal line (the mean of the data). In this case, the negative R-squared value suggests that the model performs very poorly and provides no explanatory power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Conclusion` :**\n",
    "\n",
    "- The model's predictions are far off from the actual values, as indicated by the high MSE.\n",
    "\n",
    "- The negative R-squared value suggests that the model fails to capture any meaningful relationship between the features and the target variable.\n",
    "\n",
    "- There are likely issues with the model architecture, hyperparameters, or the data itself that need to be addressed.\n",
    "\n",
    "- Further analysis and debugging are necessary to understand why the model is performing so poorly and to improve its performance. This may involve examining the data quality, feature selection, model hyperparameters, or trying different algorithms altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-03`    Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'bootstrap': False, 'max_depth': 8, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 110}\n",
      "Best Score: 0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Generate some example data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': randint(10, 200),  # Number of trees in the forest\n",
    "    'max_depth': randint(1, 20),        # Maximum depth of the trees\n",
    "    'min_samples_split': randint(2, 20), # Minimum samples required to split a node\n",
    "    'min_samples_leaf': randint(1, 20),  # Minimum samples required at each leaf node\n",
    "    'bootstrap': [True, False],          # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Random search with cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    model, param_distributions=param_dist, n_iter=50, cv=5, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Interpretation` :**\n",
    "\n",
    "- **bootstrap -** False indicates that the random forest does not use bootstrap samples when building trees, meaning each tree is trained on the entire dataset.\n",
    "\n",
    "- **max_depth -** 8 specifies the maximum depth of each tree in the forest, controlling the complexity of the model by limiting how deep the trees can grow.\n",
    "\n",
    "- **min_samples_leaf -** 3 sets the minimum number of samples required to be at a leaf node, which helps prevent overfitting by enforcing a constraint on the number of samples in leaf nodes.\n",
    "\n",
    "- **min_samples_split -** 2 defines the minimum number of samples required to split an internal node, regulating the creation of new nodes in the tree.\n",
    "\n",
    "- **n_estimators -** 110 determines the number of trees in the forest, which influences the diversity and robustness of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Conclusion` :**\n",
    "\n",
    "`Based on the random search results`, the optimal configuration for the random forest classifier involves not using bootstrap samples, limiting the maximum depth of trees to 8, setting the minimum number of samples per leaf to 3, requiring at least 2 samples to split a node, and utilizing 110 trees in the forest. With this configuration, the model achieves a high accuracy score of 90%. These hyperparameters can be used to train a final model for deployment, providing a balance between model complexity and performance on the given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-04`    What is a weak learner in Gradient Boosting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`In the context of Gradient Boosting`, a weak learner refers to a simple model that performs slightly better than random guessing on a given problem. The term \"weak\" does not imply that the model is inherently poor but rather that it's not highly accurate on its own. Typically, weak learners are simple decision trees with a shallow depth or other simple models like linear regression.**\n",
    "\n",
    "`In Gradient Boosting`, weak learners are sequentially added to the ensemble, with each subsequent model attempting to correct the errors made by the previous ones. By combining the predictions of multiple weak learners, Gradient Boosting builds a strong ensemble model that can make highly accurate predictions. The key idea is that each weak learner focuses on the mistakes of the ensemble up to that point, gradually improving the overall performance. This process continues until a stopping criterion is met or until a predefined number of weak learners have been added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-05`    What is the intuition behind the Gradient Boosting algorithm?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`The intuition behind Gradient Boosting` :**\n",
    "\n",
    "**Imagine we have a team of learners, not the best individually, but together they can be powerful.** This is the core idea of Gradient Boosting. It builds an ensemble of weak learners, like decision trees, sequentially. Each learner focuses on improving the predictions of the previous ones by targeting their errors. \n",
    "\n",
    "**`Here's how it breaks down` :**\n",
    "\n",
    "1. **Start Simple:** Begin with a basic model, like predicting the average target value for all data points.\n",
    "2. **Identify Errors:** Analyze the errors (difference between prediction and actual value) from the first model.\n",
    "3. **Train the Next Learner:** Build a new model focused on correcting those errors. This model predicts the errors themselves, not the final outcome. \n",
    "4. **Combine Predictions:**  Add the predictions from both models. The new model essentially refines the initial guess by learning from the mistakes.\n",
    "5. **Repeat and Improve:**  Repeat steps 2-4, with each new model targeting the cumulative errors of all previous models. The ensemble becomes progressively better at fitting the data.\n",
    "\n",
    "**`Key Point` : Using Gradients**\n",
    "\n",
    "We use the term \"gradient\" because it refers to the direction of steepest descent in a loss function (a mathematical way to measure error). By focusing on these gradients, each model takes a small step towards minimizing the overall error.\n",
    "\n",
    "**`Benefits` :**\n",
    "\n",
    "* **Increased Accuracy:** Ensemble learning generally leads to better predictions than individual weak learners.\n",
    "* **Flexibility:** Gradient Boosting can handle various data types and problems (regression, classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-06`    How does Gradient Boosting algorithm build an ensemble of weak learners?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Gradient Boosting is an ensemble learning technique that builds an ensemble of weak learners`, typically decision trees, in a sequential manner. The basic idea behind Gradient Boosting is to combine multiple weak learners to create a strong learner, by focusing on the mistakes made by the previous learners.**\n",
    "\n",
    "**`Here's a high-level overview of how Gradient Boosting builds its ensemble` :**\n",
    "\n",
    "1. **Initialization -** Gradient Boosting starts with an initial model, often a simple one like a single leaf. This initial model is referred to as the base learner or weak learner.\n",
    "\n",
    "2. **Fitting Weak Learners -** In each iteration, a weak learner (typically a decision tree) is fit to the data. This weak learner is trained to correct the errors made by the ensemble of models built so far. The goal is to find the weak learner that best fits the residuals (the differences between the target values and the predictions of the ensemble model).\n",
    "\n",
    "3. **Gradient Calculation -** The gradient of the loss function with respect to the predictions of the ensemble model is calculated. This gradient represents the direction and magnitude of the error that needs to be corrected.\n",
    "\n",
    "4. **Update Ensemble Model -** The weak learner found in this iteration is added to the ensemble model with a weight, which represents how much its predictions contribute to the final prediction. The weight is calculated using a process called line search, which optimizes the contribution of the weak learner to minimize the overall loss function.\n",
    "\n",
    "5. **Update Residuals -** After updating the ensemble model, the residuals (the differences between the target values and the predictions of the ensemble model) are updated by subtracting the predictions made by the newly added weak learner.\n",
    "\n",
    "6. **Repeat -** Steps 2-5 are repeated for a fixed number of iterations or until a specified stopping criterion is met (e.g., when the improvement in the loss function becomes negligible).\n",
    "\n",
    "7. **Final Ensemble -** The final ensemble model is formed by combining all the weak learners, with each weak learner weighted according to its contribution to minimizing the loss function.\n",
    "\n",
    "`By iteratively fitting weak learners to the residuals of the ensemble model and updating the ensemble to minimize the loss function`, Gradient Boosting effectively builds a strong ensemble model that can generalize well to unseen data. This approach allows Gradient Boosting to achieve high predictive performance, especially in structured/tabular data problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-07`    What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the mathematical intuition behind Gradient Boosting involves several steps.** **`Here's a simplified overview` :**\n",
    "\n",
    "1. **Decision Trees -** Start with understanding decision trees, which are the basic building blocks of Gradient Boosting. Decision trees partition the feature space into regions and assign a prediction to each region.\n",
    "\n",
    "2. **Gradient Descent -** Familiarize yourself with gradient descent optimization. Gradient descent is an optimization algorithm used to minimize a loss function by iteratively moving in the direction of steepest descent.\n",
    "\n",
    "3. **Boosting -** Understand the concept of boosting, which is an ensemble learning technique that combines weak learners (often decision trees) to create a strong learner. Boosting methods iteratively train weak models and give more weight to instances that are misclassified by the previous models.\n",
    "\n",
    "4. **Loss Functions -** Learn about different loss functions, such as squared error loss (for regression problems) or exponential loss (for classification problems). These loss functions quantify the difference between predicted and actual values.\n",
    "\n",
    "5. **Gradient Boosting Algorithm -**\n",
    "    \n",
    "    - Initialize a model with a constant value (usually the mean for regression problems, or a constant for classification problems).\n",
    "    \n",
    "    - For each iteration:\n",
    "    \n",
    "        - Compute the negative gradient of the loss function with respect to the current model's prediction. This represents the \"residuals\" or errors that the current model makes.\n",
    "    \n",
    "        - Fit a weak learner (typically a decision tree) to the negative gradient. This step determines how to adjust the model to reduce the residuals.\n",
    "    \n",
    "        - Update the model by adding a scaled version of the weak learner's prediction to the current model. The scaling factor is determined through a line search or other optimization technique.\n",
    "    \n",
    "    - Repeat the iteration process until a predefined number of trees is reached, or until convergence criteria are met.\n",
    "    \n",
    "    - The final model is the sum of all weak learners.\n",
    "\n",
    "6. **Regularization -** Understand how regularization techniques, such as shrinkage (learning rate) and tree depth, are applied to prevent overfitting during the boosting process.\n",
    "\n",
    "7. **Prediction -** Once the boosting process is complete, predictions are made by summing the predictions of all weak learners.\n",
    "\n",
    "8. **Hyperparameters Tuning -** Learn about the hyperparameters associated with Gradient Boosting, such as learning rate, tree depth, number of iterations, and regularization parameters. Tuning these hyperparameters is essential for optimal performance.\n",
    "\n",
    "`By understanding these steps and concepts`, we can develop a solid mathematical intuition for Gradient Boosting algorithms. It's also helpful to implement Gradient Boosting from scratch or using libraries like scikit-learn to deepen your understanding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
